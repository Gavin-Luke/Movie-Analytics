Welcome to my independent analysis of movie profitability! I hope you enjoy.

Obligatory package loadings
```{r}
library(glmnet)
library(priceR)
library(ggplot2)
library(lubridate)
library(dplyr)
library(tidyr)
library(pscl)
library(VIM)

```


I generated the CSV from webscraping The Movie Database (TMDB). You can find the spider in the repository in my Github, linked here: https://github.com/Gavin-Luke/Movie-Analytics
Remember to get a TMDB API key to replicate it. 

```{r}
df <- read.csv("movies.csv")

df <- subset(df, revenue != 0)

```
The code drops movies with revenue unknown. Movies on streaming services or TV movies don't have revenue we can directly analyze, so we can't glean anything from them. However, I think it's still worth it to estimate budgets for some of the movies. We're going to use K Nearest Neighbors, an unsupervised learning technique that estimates values based on how close they are to similar movies.

```{r}
df <- df %>%
  mutate(budget = ifelse(budget == 0, NA, budget))

# Apply KNN imputation only to the budget column
df <- kNN(df, variable = "budget", k = 5, imp_var = FALSE)
```


This code splts our very ugly genres column into nice dummy variables to play around with.
```{r}
df %>%
  # Split the 'genres' column into separate rows for each genre
  separate_rows(genres, sep = ",") %>%
  # Trim any extra whitespace from genre names
  mutate(genres = trimws(genres)) %>%
  # Create dummy variables (1 for presence, 0 for absence) for each genre
  mutate(value = 1) %>%
  pivot_wider(names_from = genres, values_from = value, values_fill = list(value = 0)) -> df

# View the new dataframe
head(df)


```


Release dates are considered to be very important for movie revenue. I created dummies for if a movie premeried in winter, spring, summer, or fall. 
```{r}
df <- df %>%
  mutate(release_date = as.Date(release_date)) %>%
  # Extract month from the release_date
  mutate(month = month(release_date)) %>%
  # Create a 'season' variable based on the month
  mutate(season = case_when(
    month %in% c(12, 1, 2) ~ "Winter",
    month %in% c(3, 4, 5)  ~ "Spring",
    month %in% c(6, 7, 8)  ~ "Summer",
    month %in% c(9, 10, 11) ~ "Fall"
  )) %>%
  # Create dummy variables for each season
  mutate(
    is_winter = ifelse(season == "Winter", 1, 0),
    is_spring = ifelse(season == "Spring", 1, 0),
    is_summer = ifelse(season == "Summer", 1, 0),
    is_fall   = ifelse(season == "Fall", 1, 0)
  )
```

TMDB has revenue information on movies that are still in theaters and are therefore still earning money. This drops movies released 30 days before access. 
```{r}
current_date <- Sys.Date()

# Calculate the cutoff date (one month before the current date)
cutoff_date <- current_date - lubridate::days(30)  # Adjust for 30 days to account for one month

df$release_date <- as.Date(df$release_date)

# Filter the dataframe to keep only movies released after the cutoff date
trimmed_df <- df[df$release_date > cutoff_date, ]
```

Now, we start to adjust budget and revenue for inflation, so all of the movies are weighted on the same scale. All of the money information will be in 2023 dollars.
```{r}

df$release_year <- year(df$release_date)
df$adjusted_budget <- adjust_for_inflation(df$budget, df$release_year, "United States", to_date="2023")
df$adjusted_revenue <- adjust_for_inflation(df$revenue, df$release_year, "United States", to_date="2023")

```

Visualizing revenue over the years
```{r}
df_summary <- df %>%
  group_by(release_year) %>%
  summarize(avg_adjusted_revenue = mean(adjusted_revenue, na.rm = TRUE))

# Create the line plot
ggplot(df_summary, aes(x = release_year, y = avg_adjusted_revenue)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red") +
  labs(title = "Average Adjusted revenue Over Time",
       x = "Release Year",
       y = "Average Adjusted revenue") +
  theme_minimal()
```


Now let's take a look at profitable genres
```{r}
# First, melt the genre columns to long format (if one-hot encoded)
df_long <- df %>%
  pivot_longer(cols = c(Mystery, Thriller, Drama, Crime, Romance, Action, Comedy, 
                        Animation, Family, Fantasy, Horror, Adventure, War, 
                        History, Western, Music, Documentary), 
               names_to = "genre", 
               values_to = "is_genre") %>%
  filter(is_genre == 1)

# Summarize total adjusted revenue by genre
df_genre_revenue <- df_long %>%
  group_by(genre) %>%
  summarize(total_adjusted_revenue = sum(adjusted_revenue, na.rm = TRUE))

# Plot the total adjusted revenue by genre
library(ggplot2)

ggplot(df_genre_revenue, aes(x = reorder(genre, -total_adjusted_revenue), 
                             y = total_adjusted_revenue)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Total Adjusted Revenue by Genre",
       x = "Genre", y = "Total Adjusted Revenue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

As we move into building models, let's calculate the correlation between adjusted_revenue and the numerical variables we have.
```{r}
numerical_vars <- df %>%
  select(where(is.numeric)) # Select only numeric columns

correlation_matrix <- cor(numerical_vars, use = "complete.obs") # Use 'complete.obs' to handle missing values

cor_with_revenue <- correlation_matrix["adjusted_revenue", ] # Extract correlations for 'adjusted_revenue'

library(corrplot)
corrplot(correlation_matrix, method = "circle")

```

```{r}
library(reshape2)
library(ggplot2)

# Convert correlation matrix to a long format
cor_long <- melt(correlation_matrix)

# Plot
ggplot(cor_long, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Heatmap", fill = "Correlation")

```


Now, we break into econometrics. For the first model, let's regress adjusted_revenue on everything we got. 
```{r}
model1 <- lm(adjusted_revenue ~ adjusted_budget + runtime + Mystery + Thriller + Drama + Crime + Romance + Action + Comedy + Animation + Family + Fantasy + Horror + Adventure + War + History + Western + Music + Documentary + is_winter + is_fall + is_summer + rating, data=df)
summary(model1)
```

Let's drop some variables that are not statistically significant and add in polynomials for budget and runtime
```{r}
model2 <- lm(adjusted_revenue ~ adjusted_budget + runtime + I(runtime^2) + I(adjusted_budget^2) + Mystery + Thriller + Drama + Crime + Action + Comedy + Animation + Fantasy + Adventure + War + History + Western + is_spring + is_fall + is_summer + rating, data=df)
summary(model2)
```
This does a little better, but we have a lot of variables. Let's run Lasso and Ridge regressions

```{r}

df <- na.omit(df)
# Let's get our independent variables into a matrix
X <- as.matrix(df[, c("adjusted_budget", "runtime", "Mystery", "Thriller", "Drama", "Crime", "Romance", 
                      "Action", "Comedy", "Animation", "Family", "Fantasy", "Horror", "Adventure", 
                      "War", "History", "Western", "Music", "Documentary", "rating", "is_winter",     "is_fall", 
                      "is_summer")])

# Add polynomial terms
X <- cbind(X, I(df$runtime^2), I(df$adjusted_budget^2))

# Dependent variable (revenue) as a vector
y <- df$adjusted_revenue

```

Here's the lasso:
```{r}
lasso_model <- glmnet(X, y, alpha = 1)

# Use cross-validation to find the best lambda
cv_lasso <- cv.glmnet(X, y, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min

# Fit the final Lasso model with the best lambda
lasso_final <- glmnet(X, y, alpha = 1, lambda = best_lambda_lasso)

# Look at those coefficients ðŸ”
coef(lasso_final)

```
And the Ridge:
```{r}
# Fit Ridge regression (alpha = 0)
ridge_model <- glmnet(X, y, alpha = 0)

# Cross-validation for the best lambda
cv_ridge <- cv.glmnet(X, y, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min

# Fit the final Ridge model with the best lambda
ridge_final <- glmnet(X, y, alpha = 0, lambda = best_lambda_ridge)

# Check out those Ridge coefficients
coef(ridge_final)

```

```{r}
# Fit the Lasso and Ridge models using cross-validated lambdas
cv_lasso <- cv.glmnet(X, y, alpha = 1)
lasso_final <- glmnet(X, y, alpha = 1, lambda = cv_lasso$lambda.min)

cv_ridge <- cv.glmnet(X, y, alpha = 0)
ridge_final <- glmnet(X, y, alpha = 0, lambda = cv_ridge$lambda.min)

# Predict the values using the models
lasso_predictions <- predict(lasso_final, newx = X)
ridge_predictions <- predict(ridge_final, newx = X)

# Calculate R-squared for Lasso
SST <- sum((y - mean(y))^2)  # Total sum of squares
SSE_lasso <- sum((y - lasso_predictions)^2)  # Sum of squared errors for Lasso
R2_lasso <- 1 - (SSE_lasso / SST)

# Calculate R-squared for Ridge
SSE_ridge <- sum((y - ridge_predictions)^2)  # Sum of squared errors for Ridge
R2_ridge <- 1 - (SSE_ridge / SST)

# Print R-squared values
cat("Lasso R-squared: ", R2_lasso, "\n")
cat("Ridge R-squared: ", R2_ridge, "\n")

```
Ok, the models did alright. A Lasso can give a decent model of predicting revenue.


What I'm more interested in is making a movie that can break even (BE), i.e be profitable. The "rule of thumb" that is frequently used is a movie breaks even if it makes 2.5 times its production budget. Let's make a dummy variable calculating that. 
```{r}
df <- df %>%
  mutate(BE = ifelse(revenue >= 2.5 * budget, 1, 0))
```

Let's see what percentage of movies break even
```{r}
BE_number <- round(sum(df$BE)/length(df),2)
print(sprintf("The percentage of movies that have broken even and been profitable is %.2f%%", BE_number))
```

```{r}
df$release_date <- as.Date(df$release_date)

# Extract the year from release_date
df <- df %>%
  mutate(release_year = format(release_date, "%Y"))

# Filter and count movies with BE = 1 over time
movies_BE <- df %>%
  filter(BE == 1) %>%                 # Only include movies that break even
  group_by(release_year) %>%          # Group by release year
  summarise(count_BE = n())           # Count the number of movies per year

# Plot the data
ggplot(movies_BE, aes(x = release_year, y = count_BE)) +
  geom_line(color = "blue", size = 1) + # Line chart
  geom_point(color = "red", size = 2) + # Add points for emphasis
  labs(
    title = "Number of Break-Even Movies Over Time",
    x = "Release Year",
    y = "Count of BE = 1 Movies"
  ) +
  theme_minimal() +                    # Minimal theme for clean visuals
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels
    plot.title = element_text(hjust = 0.5)            # Center the title
  )



```



Now here's a Linear Probability Model with BE as our dependent, and our trusty covariates returning for the sequel
```{r}
model3 <- lm(BE ~ adjusted_budget + runtime + Mystery + Thriller + Drama + Crime + Romance + Action + Comedy + Animation + Family + Fantasy + Horror + Adventure + War + History + Western + Music + Documentary + rating + is_winter + is_fall + is_summer, data=df)
summary(model3)
```
Tis model is awful (see the R-Squared). Let's try Probit and Logit, they're better for this type of task. It's also going to calculate McFadden's Pseudo R-Squared to check how good the model is. It's interpreted similar to a normal R-Squared. 

```{r}
logit_model <- glm(BE ~ adjusted_budget + I(adjusted_budget^2) + runtime + I(runtime^2) + Mystery + Thriller + Drama + Crime + Romance + Action + Comedy + Animation + Family + Fantasy + Horror + Adventure + War + History + Western + Music + Documentary + is_winter + is_fall + is_summer + rating, data=df, family = binomial(link = "logit"))
summary(logit_model)
print(pR2(logit_model)['McFadden'])
```
McFadden's R-Squared is awful. This model doesn't tell us anything.
And the Probit

```{r}
probit_model <- glm(BE ~ adjusted_budget + I(adjusted_budget^2) + runtime + I(runtime^2) + Mystery + Thriller + Drama + Crime + Romance + Action + Comedy + Animation + Family + Fantasy + Horror + Adventure + War + History + Western + Music + Documentary + rating + is_winter + is_fall + is_summer, data=df, family = binomial(link = "probit"))
summary(probit_model)
print(pR2(probit_model)['McFadden'])
```
The McFadden's is also awful here, but let's push forward anyway. 
Let's see if the probit or the logit is better at predicting our movies
```{r}
# 1. Predict probabilities using the models
logit_probs <- predict(logit_model, type = "response")  # For Logit
probit_probs <- predict(probit_model, type = "response")  # For Probit

# 2. Convert probabilities to binary predictions (threshold = 0.5)
logit_preds <- ifelse(logit_probs >= 0.5, 1, 0)  # Logit predictions
probit_preds <- ifelse(probit_probs >= 0.5, 1, 0)  # Probit predictions

# 3. Create a confusion matrix for Logit
logit_correct <- sum(logit_preds == df$BE)  # Count correct predictions
logit_total <- nrow(df)  # Total number of observations

# 4. Create a confusion matrix for Probit
probit_correct <- sum(probit_preds == df$BE)  # Count correct predictions
probit_total <- nrow(df)  # Total number of observations

# 5. Print out the results
cat("Logit Model:\n")
cat("Correct Predictions: ", logit_correct, "/", logit_total, " (", 
    round((logit_correct / logit_total) * 100, 2), "%)\n", sep = "")
    
cat("Probit Model:\n")
cat("Correct Predictions: ", probit_correct, "/", probit_total, " (", 
    round((probit_correct / probit_total) * 100, 2), "%)\n", sep = "")

```
Our probit just inches out the logit. Again, these models are still not fantastic, but what can you do?



Now, let's studio produce the movie most likely to break even. We'll assume a fixed budget and runtime at the mean of both, pick two genres and a release date, and it'll break even! The section below uses the probit model to create predictions for which are the most and least likely movies to break even. 
```{r}
# Define your fixed budget
fixed_budget <- mean(df$adjusted_budget)
fixed_runtime <- mean(df$runtime)
fixed_rating <- mean(df$rating)

# Define possible genres and seasons
genres <- c("Mystery", "Thriller", "Drama", "Crime", "Romance", "Action", "Comedy", 
            "Animation", "Family", "Fantasy", "Horror", "Adventure", "War", "History", 
            "Western", "Music", "Documentary")

seasons <- c("is_fall", "is_winter", "is_summer", "none")  # none represents no season chosen

# Create combinations of 2 genres
genre_combinations <- combn(genres, 2, simplify = FALSE)

# Create a dataframe of all possible combinations of 2 genres and 1 season
library(tidyr)

combinations_df <- expand.grid(genre_combinations = genre_combinations, 
                               season = seasons, stringsAsFactors = FALSE)

# Split the genre_combinations into separate columns for the two genres
combinations_df <- combinations_df %>%
  mutate(genre1 = sapply(genre_combinations, `[`, 1),
         genre2 = sapply(genre_combinations, `[`, 2)) %>%
  select(-genre_combinations)

```


```{r}
# Initialize an empty vector to store the probabilities
combinations_df$predicted_probability <- NA

# Loop through each combination and predict the probability using the probit model
for (i in 1:nrow(combinations_df)) {
  
  # Create a temporary dataframe for this combination
  temp_df <- data.frame(
    adjusted_budget = fixed_budget,
    runtime = fixed_runtime,
    rating = fixed_rating,
    Mystery = 0, Thriller = 0, Drama = 0, Crime = 0, Romance = 0, Action = 0,
    Comedy = 0, Animation = 0, Family = 0, Fantasy = 0, Horror = 0, Adventure = 0, 
    War = 0, History = 0, Western = 0, Music = 0, Documentary = 0,
    is_fall = 0, is_winter = 0, is_summer = 0
  )
  
  # Set the selected genres to 1
  temp_df[[combinations_df$genre1[i]]] <- 1
  temp_df[[combinations_df$genre2[i]]] <- 1
  
  # Set the selected season to 1 (if not "none")
  if (combinations_df$season[i] != "none") {
    temp_df[[combinations_df$season[i]]] <- 1
  }
  
  # Predict the probability of breaking even using the probit model
  combinations_df$predicted_probability[i] <- predict(probit_model, newdata = temp_df, type = "response")
}

```

```{r}
# Sort the dataframe by predicted probability in descending order
sorted_combinations_df <- combinations_df[order(-combinations_df$predicted_probability),]

# Get the top 5 combinations with the highest predicted probability
top_5_combinations <- head(sorted_combinations_df, 15)

# Print the top 5 combinations
print(top_5_combinations)


```
```{r}
bottom_5_combinations <- tail(sorted_combinations_df, 15)

# Print the top 5 combinations
print(bottom_5_combinations)
```

